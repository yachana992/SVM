# -*- coding: utf-8 -*-
"""SVM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uzZEH5jf6XEJUM0FArnGLpGrh2Z7r8U7
"""

#Import required Libraries
import re
import nltk
import numpy as np
from sklearn.feature_extraction import DictVectorizer
from sklearn.model_selection import train_test_split
import pandas as pd
from random import shuffle

nltk.download('punkt')

#To access the dataset 
def dataset(folderpath):
    input_file = open(folderpath, "r")
    lines = []
    for line in input_file.readlines():
    	line = line.strip("\n")
    	line_temp = line.split(' ')
    	line= re.sub(r'</Sentence>',"", line)
    	line = re.sub(r'<Sentence id=[0-9]+>',"",line)
    	if len(line) != 0 and line_temp[0]:
    		lines.append(line)
    return lines

#Extracting the features for each tokens
def get_features(tokens, index):
    word = tokens[index]

    if index == 0:
        prevword = ''
        prevprevword = ''
    elif index == 1:
        prevword = tokens[index - 1]
        prevprevword = ''
    else:
        prevword = tokens[index-1]
        prevprevword = tokens[index - 2]

    if index == len(tokens) - 1:
        nextword = ''
        nextnextword = ''

    elif index == len(tokens) - 2:
        nextword = tokens[index + 1]
        nextnextword = ''

    else:
        nextword= tokens[index + 1]
        nextnextword = tokens[index + 2]

    return {
            'word': word,
            # 'pos': '',

            'next-word': nextword,
            # 'next-pos': '',

            'next-next-word': nextnextword,
            # 'nextnextpos': '',

            'prev-word': prevword,
            # 'prev-pos': '',

            'prev-prev-word': prevprevword,
            # 'prev-prev-pos': '',
    }

#Split the sentences into list of words where each word is a pair of a word and a tag.
def sentences_to_words(sentence):
    """
    Arguments:
    news = sentence to be tokenized

    Output:
    tokens = list of words in a sentence
    tags = list of tags of the words in a sentence
    """
    sentences = []
    tokens = []
    tags = []

    # use the NLTK tokenizer to split the sentences into words
    raw_sentences = nltk.word_tokenize(sentence)

    #Splitting each words into a word and a tag
    for raw_sentence in raw_sentences:
        sentence = raw_sentence.split('_')
        sentences.append(sentence)
    for word in sentences:
    	tokens.append(word[0])
    	tags.append(word[-1])

    return tokens, tags

#Class SVM
class SVM:
    #Initializing the class
    def __init__(self, learning_rate=0.001, lambda_param=0.01, n_iters=1000):
        self.lr = learning_rate
        self.lambda_param = lambda_param
        self.n_iters = n_iters
        self.w = None
        self.b = None

    #Fitting the model
    def fit(self, X, y_):
        n_samples, n_features = X.shape
        
        self.w = np.zeros(n_features)
        self.b = 0
        #Iteration
        for _ in range(self.n_iters):
            for idx, x_i in enumerate(X):
                condition = y_[idx] * (np.dot(x_i, self.w) - self.b) >= 1
                if condition:
                    self.w -= self.lr * (2 * self.lambda_param * self.w)
                else:
                    self.w -= self.lr * (2 * self.lambda_param * self.w - np.dot(x_i, y_[idx]))
                    self.b -= self.lr * y_[idx]

    #Using the built model to predict
    def predict(self, X):
        approx = np.dot(X, self.w) - self.b
        return approx

#Loading the dataset
raw_data = dataset("data/nepali_universal_v4.pos")

raw_data = raw_data[1:]

train_sentences = []  # Initialize an empty list of sentences
train_features = []
label = []

for data in raw_data:
  train_tokens, train_tags = sentences_to_words(data)
  label += train_tags
  for x in range(len(train_tokens)):
    train_features.append(get_features(train_tokens,x))

#Printing the feature dictionary and the labels
print(train_features)
print(label)

#Converting the feature arrays into feature vectors)
vec = DictVectorizer(sparse=False)
X_arr = vec.fit_transform(train_features)
print(X_arr.shape)

y = np.array(label)

X_train, X_test, y_train, y_test = train_test_split(X_arr, y, test_size=0.33, random_state=42)

possible_tags = set(y_train)
models = {}

# We train models for all the possible labels we have seen in train data.
# If there are N unique labels in train data, we will have N models to be trained.
for t in possible_tags:
    models[t] = SVM()
    y_training = np.asarray([1 if i == t else -1 for i in y_train])
    #Fitting data into each model in the dictionary
    models[t].fit(X_train, y_training)

#Function returns a tag for a given word. It calculates score for each tag and returns tag with maximum score.
def pred_tag(word):
    scores = {}
    for k, v in models.items():
        scores[k] = v.predict(word)
    return sorted(scores.items(), key=lambda kv: kv[1], reverse=True)[0][0]

#Predict tags for each word
pred_tags = [pred_tag(test_word) for test_word in X_test]
print()
print(pred_tags)

print(len(pred_tags))

#Funtion to calculate the accuracy
def accuracy(y_true, y_pred):
    accuracy = np.sum(y_true == y_pred) / len(y_true)
    return accuracy

y_pred = np.array(pred_tags)

#Calculating the accuracy of the model
print(accuracy(y_test,y_pred))
